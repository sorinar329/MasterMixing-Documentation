\chapter{Conclusion}
\label{chap:conclusion}

\section{Summary}
This master's thesis serves as a proof of concept to demonstrate how a robotic system like the \textit{PR2} \cite{pr2}, utilizing the cognitive architecture \textit{CRAM} \cite{beetz10cram}, can make knowledge-based decisions and perform mixing actions in a simulation.

In order to implement the motions in \textit{CRAM} \cite{beetz10cram}, one must first understand the relationship of the motions to the overall context of the task the robot needs to solve. What we have discovered, primarily conducted through video analyses of various cooking videos, is that the motions are primarily dependent on the ingredients in the context of mixing task, which variations we highlighted in our work. The containers and tools used do not directly influence the motion itself, but they do modify the action range of the motions
(see \nameref{chap:Data_acquisition}).

The model we use—a knowledgebase (see \nameref{chap:Data_representation}) —captures various important aspects of a mixing action. It includes information about the ingredients being mixed, the types of motions that can be performed, the type of container used for storing and mixing ingredients, and the tools needed for mixing. By keeping the model simple, we ensure that the robot can infer motions using the rule-based language \textit{SWRL} (\ref{sec:SWRL}).

Our modeled motions do not specify how a motion is executed but provide capabilities via parameters to adapt to different containers and tools. The implementation of each motion—\textbf{Circular}, \textbf{Folding}, \textbf{Horizontal Elliptical}, and \textbf{Whirlstorm}—is available in \textit{PyCram} \cite{pycram}, and any robotic system that can use \textit{CRAM} \cite{beetz10cram} can perform these motions.

We also transitioned from theory to practice and implemented and tested the \textit{Mixing Action Designator}, 
which actively uses the \textit{mixing} ontology. Through evaluation, we tested as many scenarios as possible to demonstrate how the 
ontology infers different motions, which the robot executed (see \nameref{chap:Execution on the robot}).

In addition to our proof of concept, we developed a different way to visualize ontologies by focusing on complex class expressions, which hasn't been done before (see \nameref{chap:OWLViz}). 
With the \textit{Knowledge Graph Visualization Tool}, a user can query complex class expressions without needing to know how to write \textit{SPARQL} queries. 
The \textit{Knowledge Graph Visualization Tool} generates a \textit{SPARQL} query for the user, using triple matching to get the desired graph. 
This query can be customized by changing its contents without altering the triple matching. Another feature of the \textit{Knowledge Graph Visualization Tool}
is the \textit{Custom Inference}, which reasons over the \texit{mixing} ontology, to infer motions from task and ingredients. As a result a visualized graph 
and a sequence of actions are generated, to highlight the reasoning.  

\section{Future Work}
This proof of concept was developed in \textit{CRAM} \cite{beetz10cram} in simulation. Simulations simplify many aspects: the robot has ground truth for each available object, the ingredients to mix, the tools for mixing, and the container to mix in. To advance our work, this proof of concept should be implemented on a real robot to assess the increased challenges in real-world scenarios.

Regarding future work, several limitations emerged during the implementation of our proof of concept. Our motions are essentially 2D, with one axis remaining unchanged throughout the motion. Transitioning from simulation to the real world will require incorporating depth into each motion to ensure the substances are properly mixed.

Another limiting aspect is the lack of collision detection. Currently, we ensure that the tool does not collide with objects concerning only 2 out of 3 axes. If the tool is placed too low and the container has a spherical rather than a cuboid shape, the tool will eventually collide with the object. To address this limitation, we need depth-based perception and various techniques to determine the optimal height for the tool during the execution of a mixing action.

Once we transition to the real world, we lose the ground truth on all relevant objects needed for mixing. We will need perception frameworks capable of detecting and locating objects in a scene, picking and placing objects, and performing various actions, such as pouring, to fill a container with ingredients.

In simulation, the robot will not fail unless the inference fails. As long as a motion is inferred, the mixing will be executed. This approach needs to be adjusted for real-world application.

One last important aspect is the implementation of failure handling, which is currently completely absent. Failure handling should address various aspects of robot actions, such as scenarios where the robot plan cannot proceed due to reasons like incorrect perception or grasping of an object. Countermeasures should be developed for such potential scenarios to still allow plan execution.
